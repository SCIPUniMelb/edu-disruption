edges <- bind_rows(egde1, egde2_he, egde2_she)
p <- nodes %>%
ggplot(aes(x, y, label = word, size = n)) +
geom_text(hjust = 0, color = "#DDDDDD") +
theme_void() +
geom_line(data = edges,
aes(x, y, group = id, color = from, alpha = sqrt(number)),
inherit.aes = FALSE) +
theme(plot.background = element_rect(fill = "#666666", colour = 'black'),
text = element_text(color = "#EEEEEE", size = 15)) +
guides(alpha = "none", color = "none", size = "none") +
xlim(c(0.9, 3.2)) +
scale_color_manual(values = c("#8524A5", "#F79341")) +
labs(title = " Vizualizing trigrams in Sherlock Holmes") +
scale_size(range = c(3, 8))
p
library(details)
sessioninfo::session_info() %>%
details::details(summary = 'session information')
# set-up url for scraping
march20_archive = "https://visualisingideas.edublogs.org/2020/03/"
# Read the HTML code from the website
march_posts <- read_html(march20_archive)
library(tidyverse)
library(rvest)
# function to remove html tags
cleanFun = function(htmlString) {
return(gsub("<.*?>", "", htmlString))
}
# set-up url for scraping
march20_archive = "https://visualisingideas.edublogs.org/2020/03/"
# Read the HTML code from the website
march_posts <- read_html(march20_archive)
## install packages
install.packages(c("tidyverse","tidytext","rtweet"), dep = TRUE)
library(tidyverse)
library(tidytext)
library(rtweet)
# include API keys
source("keys.R")
install.packages(c("tidyverse", "tidytext", "rtweet"), dep = TRUE)
## search for tweets
rl_tweets <- search_tweets(q = "#remotelearning", n = 18000, include_rts = FALSE, `-filter` = "replies", lang = "en")
## install packages
# install.packages(c("tidyverse","tidytext","rtweet"), dep = TRUE)
library(tidyverse)
library(tidytext)
library(rtweet)
# include API keys
source("keys.R")
## search for tweets
rl_tweets <- search_tweets(q = "#remotelearning", n = 18000, include_rts = FALSE, `-filter` = "replies", lang = "en")
rl_tweets %>%
sample_n(5) %>%
select(created_at, screen_name, text, favorite_count, retweet_count)
write_as_csv(rl_tweets, "tweets-remotelearning.csv")
ts_plot(tweets, "hours") +
labs(x = NULL, y = NULL,
title = "Frequency of tweets with #COVID hashtag",
subtitle = paste0(format(min(tweets$created_at), "%d %B %Y"), " to ", format(max(tweets$created_at),"%d %B %Y")),
caption = "Data collected from Twitter's REST API via rtweet") +
theme_minimal()
ts_plot(rl_tweets, "hours") +
labs(x = NULL, y = NULL,
title = "Frequency of tweets with #remotelearning hashtag",
subtitle = paste0(format(min(rl_tweets$created_at), "%d %B %Y"), " to ", format(max(rl_tweets$created_at),"%d %B %Y")),
caption = "Data collected from Twitter's REST API via rtweet") +
theme_minimal()
rl_tweets %>%
arrange(-retweet_count) %>%
slice(1) %>%
select(created_at, screen_name, text, retweet_count)
rl_tweets %>%
count(screen_name, sort = TRUE) %>%
top_n(10) %>%
mutate(screen_name = paste0("@", screen_name))
rl_tweets %>%
unnest_tokens(hashtag, text, "tweets", to_lower = FALSE) %>%
filter(str_detect(hashtag, "^#"),
hashtag != "#ClimateEmergency") %>%
count(hashtag, sort = TRUE) %>%
top_n(10)
rl_tweets %>%
arrange(-favorite_count) %>%
top_n(5, favorite_count) %>%
select(created_at, screen_name, text, favorite_count)
# install.packages("devtools")
devtools::install_github("hadley/emo")
library(emo)
rl_tweets %>%
mutate(emoji = ji_extract_all(text)) %>%
unnest(cols = c(emoji)) %>%
count(emoji, sort = TRUE) %>%
top_n(10)
install.packages("devtools")
devtools::install_github("hadley/emo")
library(emo)
install.packages("devtools")
rl_tweets %>%
mutate(emoji = ji_extract_all(text)) %>%
unnest(cols = c(emoji)) %>%
count(emoji, sort = TRUE) %>%
top_n(10)
words <- rl_tweets %>%
mutate(text = str_remove_all(text, "&amp;|&lt;|&gt;"),
text = str_remove_all(text, "\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)"),
text = str_remove_all(text, "[^\x01-\x7F]")) %>%
unnest_tokens(word, text, token = "tweets") %>%
filter(!word %in% stop_words$word,
!word %in% str_remove_all(stop_words$word, "'"),
str_detect(word, "[a-z]"),
!str_detect(word, "^#"),
!str_detect(word, "@\\S+")) %>%
count(word, sort = TRUE)
words <- rl_tweets %>%
mutate(text = str_remove_all(text, "&amp;|&lt;|&gt;"),
text = str_remove_all(text, "\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)"),
text = str_remove_all(text, "[^\x01-\x7F]")) %>%
unnest_tokens(word, text, token = "rl_tweets") %>%
filter(!word %in% stop_words$word,
!word %in% str_remove_all(stop_words$word, "'"),
str_detect(word, "[a-z]"),
!str_detect(word, "^#"),
!str_detect(word, "@\\S+")) %>%
count(word, sort = TRUE)
words <- rl_tweets %>%
mutate(text = str_remove_all(text, "&amp;|&lt;|&gt;"),
text = str_remove_all(text, "\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)"),
text = str_remove_all(text, "[^\x01-\x7F]")) %>%
unnest_tokens(word, text, token = "tweets") %>%
filter(!word %in% stop_words$word,
!word %in% str_remove_all(stop_words$word, "'"),
str_detect(word, "[a-z]"),
!str_detect(word, "^#"),
!str_detect(word, "@\\S+")) %>%
count(word, sort = TRUE)
# Then we use the wordcloud package to create a visualisation of the word frequencies.
library(wordcloud)
words %>%
with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors = "#F29545"))
words <- rl_tweets %>%
mutate(text = str_remove_all(text, "&amp;|&lt;|&gt;"),
text = str_remove_all(text, "\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)"),
text = str_remove_all(text, "[^\x01-\x7F]")) %>%
unnest_tokens(word, text, token = "tweets") %>%
filter(!word %in% stop_words$word,
!word %in% str_remove_all(stop_words$word, "'"),
str_detect(word, "[a-z]"),
!str_detect(word, "^#"),
!str_detect(word, "@\\S+")) %>%
count(word, sort = TRUE)
# Then we use the wordcloud package to create a visualisation of the word frequencies.
library(wordcloud)
words %>%
with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors = "#F29545"))
View(rl_tweets)
setwd("~/Documents/GitHub/WORKING PROJECTS/edu-disruption")
library(DT)
rl_tweets %>%
sample_n(5) %>%
select(created_at, screen_name, text, favorite_count, retweet_count)
datatable(rl_tweets)
datatable(rl_tweets) %>%
sample_n(5) %>%
select(created_at, screen_name, text, favorite_count, retweet_count)
library(tidytext)
pp <- p %>%
unnest_tokens(word, value) %>%
filter(!word %in% stop_words$word ) %>%
count(word, sort = TRUE) %>%
arrange(desc(n))
library(tidyverse)
library(rvest)
library(kableExtra)
# function to remove html tags
cleanFun = function(htmlString) {
return(gsub("<.*?>", "", htmlString))
}
# set-up url for scraping
march20_archive = "https://visualisingideas.edublogs.org/2020/03/"
# Read the HTML code from the website
march_posts <- read_html(march20_archive)
march_titles <- html_nodes(march_posts,'.entry-title')   #’.entry-title’ is the blog Title
march_paras <- html_nodes(march_posts,'p')   #’p’ are the paragraphs
# Convert the data to text and do some additional cleaning up.
march_titles %>%
html_text() %>%
# remove empty white space
str_trim() %>%
# remove new line indicator \n
str_remove_all(pattern = "\n")
# Clean up the text from each blog post.
p <- march_paras %>%
# remove empty white space
html_text() %>%
# remove empty white space
str_trim() %>%
# remove new line indicator \n
str_remove_all(pattern = "\n") %>%
# remove html tags
cleanFun() %>%
as_tibble()
library(tidytext)
pp <- p %>%
unnest_tokens(word, value) %>%
filter(!word %in% stop_words$word ) %>%
count(word, sort = TRUE) %>%
arrange(desc(n))
kable(pp) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
library(tidytext)
pp <- p %>%
unnest_tokens(word, value) %>%
filter(!word %in% stop_words$word ) %>%
count(word, sort = TRUE) %>%
arrange(desc(n))
kable(pp) %>%
head(20) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
library(tidytext)
pp <- p %>%
unnest_tokens(word, value) %>%
filter(!word %in% stop_words$word ) %>%
count(word, sort = TRUE) %>%
arrange(desc(n))
kable(pp[1:10]) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
library(tidyverse)
library(rvest)
library(DT)
# function to remove html tags
cleanFun = function(htmlString) {
return(gsub("<.*?>", "", htmlString))
}
library(tidytext)
pp <- p %>%
unnest_tokens(word, value) %>%
filter(!word %in% stop_words$word ) %>%
count(word, sort = TRUE) %>%
arrange(desc(n))
DT::datatable(head(pp), editable = list(
target = 'row', disable = list(columns = c(1, 3, 4))
))
library(tidytext)
pp <- p %>%
unnest_tokens(word, value) %>%
filter(!word %in% stop_words$word ) %>%
count(word, sort = TRUE) %>%
arrange(desc(n))
DT::datatable(pp, editable = list(
target = 'row', disable = list(columns = c(1, 3, 4))
))
positive <- get_sentiments("bing") %>%
filter(sentiment == "positive")
negative <- get_sentiments("bing") %>%
filter(sentiment == "negative")
# https://dplyr.tidyverse.org/reference/filter-joins.html#examples
# To suppress the message about joining variables, supply `by`
# band_members %>% semi_join(band_instruments, by = "name")
# What are the most common positive words
pos <- pp %>%
dplyr::semi_join(positive, by = "word") %>%
select(Positive = word, pos_total = n) %>%
head(20)
# What are the most common negative words
neg <- pp %>%
dplyr::semi_join(negative, by = "word") %>%
select(Negative = word, neg_total = n) %>%
head(20)
DT::datatable(bind_cols(pos, neg), editable = list(
target = 'row', disable = list(columns = c(1, 3, 4))
))
## install packages
# install.packages(c("tidyverse","tidytext","rtweet"), dep = TRUE)
library(tidyverse)
library(tidytext)
library(rtweet)
library(DT)
# include API keys
source("keys.R")
## search for tweets
tweets <- search_tweets(q = "#remotelearning", n = 18000, include_rts = FALSE, `-filter` = "replies", lang = "en")
rl_tweets %>%
sample_n(5) %>%
select(created_at, screen_name, text, favorite_count, retweet_count)  %>%
DT::datatable(editable = list(target = 'row', disable = list(columns = c(1, 3, 4))))
tweets %>%
sample_n(5) %>%
select(created_at, screen_name, text, favorite_count, retweet_count)  %>%
DT::datatable(editable = list(target = 'row', disable = list(columns = c(1, 3, 4))))
write_as_csv(rl_tweets, "data/tweets-remotelearning.csv")
tweets %>%
sample_n(5) %>%
select(created_at, screen_name, text, favorite_count, retweet_count)  %>%
DT::datatable(editable = list(target = 'row', disable = list(columns = c(1, 3, 4))))
write_as_csv(tweets, "data/tweets-remotelearning.csv")
tweets %>%
sample_n(30) %>%
select(created_at, screen_name, text, favorite_count, retweet_count)  %>%
DT::datatable(editable = list(target = 'row', disable = list(columns = c(1, 3, 4))))
write_as_csv(tweets, "data/tweets-remotelearning.csv")
ts_plot(remote-learning, "hours") +
labs(x = NULL, y = NULL,
title = "Frequency of tweets with #remotelearning hashtag",
subtitle = paste0(format(min(tweets$created_at), "%d %B %Y"), " to ", format(max(tweets$created_at),"%d %B %Y")),
caption = "Data collected from Twitter's REST API via rtweet") + theme_minimal()
ts_plot(tweets, "hours") +
labs(x = NULL, y = NULL,
title = "Frequency of tweets with #remotelearning hashtag",
subtitle = paste0(format(min(tweets$created_at), "%d %B %Y"), " to ", format(max(tweets$created_at),"%d %B %Y")),
caption = "Data collected from Twitter's REST API via rtweet") + theme_minimal()
install.packages("emo")
devtools::install_github("hadley/emo")
library(tidyverse)
library(rvest)
library(DT)
# function to remove html tags
cleanFun = function(htmlString) {
return(gsub("<.*?>", "", htmlString))
}
# set-up url for scraping
march20_archive = "https://visualisingideas.edublogs.org/2020/03/"
# Read the HTML code from the website
march_posts <- read_html(march20_archive)
march_titles <- html_nodes(march_posts,'.entry-title')   #’.entry-title’ is the blog Title
march_paras <- html_nodes(march_posts,'p')   #’p’ are the paragraphs
# Convert the data to text and do some additional cleaning up.
march_titles %>%
html_text() %>%
# remove empty white space
str_trim() %>%
# remove new line indicator \n
str_remove_all(pattern = "\n")
# Clean up the text from each blog post.
p <- march_paras %>%
# remove empty white space
html_text() %>%
# remove empty white space
str_trim() %>%
# remove new line indicator \n
str_remove_all(pattern = "\n") %>%
# remove html tags
cleanFun() %>%
as_tibble()
# Clean up the text from each blog post.
p <- march_paras %>%
# remove empty white space
html_text() %>%
# remove empty white space
str_trim() %>%
# remove new line indicator \n
str_remove_all(pattern = "\n") %>%
# remove html tags
cleanFun() %>%
as_tibble()
library(tidytext)
pp <- p %>%
unnest_tokens(word, value) %>%
filter(!word %in% stop_words$word ) %>%
count(word, sort = TRUE) %>%
arrange(desc(n)) %>%
DT::datatable(options = list(
searching = FALSE,
pageLength = 5,
lengthMenu = c(5, 10, 15, 20)))
positive <- get_sentiments("bing") %>%
filter(sentiment == "positive")
negative <- get_sentiments("bing") %>%
filter(sentiment == "negative")
# https://dplyr.tidyverse.org/reference/filter-joins.html#examples
# To suppress the message about joining variables, supply `by`
# band_members %>% semi_join(band_instruments, by = "name")
# What are the most common positive words
pos <- pp %>%
dplyr::semi_join(positive, by = "word") %>%
select(Positive = word, pos_total = n) %>%
head(20)
library(tidytext)
pp <- p %>%
unnest_tokens(word, value) %>%
filter(!word %in% stop_words$word ) %>%
count(word, sort = TRUE) %>%
arrange(desc(n))
DT::datatable(pp, editable = list(target = 'row', pageLength = 5, lengthMenu = c(5, 10, 15, 20)))
library(tidytext)
pp <- p %>%
unnest_tokens(word, value) %>%
filter(!word %in% stop_words$word ) %>%
count(word, sort = TRUE) %>%
arrange(desc(n))
DT::datatable(pp, editable = list(target = 'row', pageLength=5, lengthMenu = c(5, 10, 15, 20)))
positive <- get_sentiments("bing") %>%
filter(sentiment == "positive")
negative <- get_sentiments("bing") %>%
filter(sentiment == "negative")
# https://dplyr.tidyverse.org/reference/filter-joins.html#examples
# To suppress the message about joining variables, supply `by`
# band_members %>% semi_join(band_instruments, by = "name")
# What are the most common positive words
pos <- pp %>%
dplyr::semi_join(positive, by = "word") %>%
select(Positive = word, pos_total = n) %>%
head(20)
# What are the most common negative words
neg <- pp %>%
dplyr::semi_join(negative, by = "word") %>%
select(Negative = word, neg_total = n) %>%
head(20)
DT::datatable(bind_cols(pos, neg), editable = list(
target = 'row', disable = list(columns = c(1, 3, 4))
))
positive <- get_sentiments("bing") %>%
filter(sentiment == "positive")
negative <- get_sentiments("bing") %>%
filter(sentiment == "negative")
# https://dplyr.tidyverse.org/reference/filter-joins.html#examples
# To suppress the message about joining variables, supply `by`
# band_members %>% semi_join(band_instruments, by = "name")
# What are the most common positive words
pos <- pp %>%
dplyr::semi_join(positive, by = "word") %>%
select(Positive = word, pos_total = n) %>%
head(20)
# What are the most common negative words
neg <- pp %>%
dplyr::semi_join(negative, by = "word") %>%
select(Negative = word, neg_total = n) %>%
head(20)
DT::datatablebind_cols(pos, neg), (editable = list(target = 'row', pageLength = 5, lengthMenu = c(5, 10, 15, 20)))
positive <- get_sentiments("bing") %>%
filter(sentiment == "positive")
negative <- get_sentiments("bing") %>%
filter(sentiment == "negative")
# https://dplyr.tidyverse.org/reference/filter-joins.html#examples
# To suppress the message about joining variables, supply `by`
# band_members %>% semi_join(band_instruments, by = "name")
# What are the most common positive words
pos <- pp %>%
dplyr::semi_join(positive, by = "word") %>%
select(Positive = word, pos_total = n) %>%
head(20)
# What are the most common negative words
neg <- pp %>%
dplyr::semi_join(negative, by = "word") %>%
select(Negative = word, neg_total = n) %>%
head(20)
DT::datatablebind_cols(pos, neg), editable = list(target = 'row', pageLength = 5, lengthMenu = c(5, 10, 15, 20)))
positive <- get_sentiments("bing") %>%
filter(sentiment == "positive")
negative <- get_sentiments("bing") %>%
filter(sentiment == "negative")
# https://dplyr.tidyverse.org/reference/filter-joins.html#examples
# To suppress the message about joining variables, supply `by`
# band_members %>% semi_join(band_instruments, by = "name")
# What are the most common positive words
pos <- pp %>%
dplyr::semi_join(positive, by = "word") %>%
select(Positive = word, pos_total = n) %>%
head(20)
# What are the most common negative words
neg <- pp %>%
dplyr::semi_join(negative, by = "word") %>%
select(Negative = word, neg_total = n) %>%
head(20)
DT::datatable(bind_cols(pos, neg), editable = list(target = 'row', pageLength = 5, lengthMenu = c(5, 10, 15, 20)))
library(shiny)
library(rtweet)
library(tidyverse)
library(scales)
# Define UI for application that draws a histogram
ui <- fluidPage(
# Application title
titlePanel("Twitter Data"),
# Sidebar with a slider input for number of bins
sidebarLayout(
sidebarPanel(
sliderInput("n",
"How many trends would you like to see?",
min = 1,
max = 25,  # helps avoid API limits of 18,000/15 mins
value = 3) # default valie
),
# Show a plot of the generated distribution
mainPanel(
plotOutput("distPlot")
)
)
)
# Define server logic required to draw a histogram
server <- function(input, output) {
trends <- get_trends("washington")
output$trends <- renderPlot({
trends[, c('trends','tweet_volume')] %>%
arrange(desc(tweet_volume)) %>%
rename(Trend = trend, Volume = tweet_volume)
# generate bins based on input$bins from ui.R
x    <- trends$Volume
bins <- seq(min(x), max(x), length.out = input$bins + 1)
# draw the plot with the specified number of bins
ggplot() +
geom_bar(mapping = aes(x = reorder(trend, -tweet_volume),
y = tweet_volume),
stat = "identity") +
scale_y_continuous(labels = comma) +
theme(axis.title.x = element_blank()) +
theme(axis.title.y = element_blank()) +
labs(title = "Washington DC Twitter Trends",
caption = "\nSource: Data collected via rtweet - graphic by @mjhendrickson")
})
}
# Run the application
shinyApp(ui = ui, server = server)
