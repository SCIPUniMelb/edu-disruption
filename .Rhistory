trends[, c('trends','tweet_volume')] %>%
arrange(desc(tweet_volume)) %>%
rename(Trend = trend, Volume = tweet_volume)
# generate bins based on input$bins from ui.R
x    <- trends$Volume
bins <- seq(min(x), max(x), length.out = input$bins + 1)
# draw the plot with the specified number of bins
ggplot() +
geom_bar(mapping = aes(x = reorder(trend, -tweet_volume),
y = tweet_volume),
stat = "identity") +
scale_y_continuous(labels = comma) +
theme(axis.title.x = element_blank()) +
theme(axis.title.y = element_blank()) +
labs(title = "Washington DC Twitter Trends",
caption = "\nSource: Data collected via rtweet - graphic by @mjhendrickson")
})
}
# Run the application
shinyApp(ui = ui, server = server)
# Sentiment analysis using tidytext
# https://www.edgarsdatalab.com/2017/09/04/sentiment-analysis-using-tidytext/
library(tidyverse)
library(here)
tweets-rl <- read_csv(here::here("Data", "tweets-remotelearning.csv"))
# Sentiment analysis using tidytext
# https://www.edgarsdatalab.com/2017/09/04/sentiment-analysis-using-tidytext/
library(tidyverse)
library(here)
tweets_rl <- read_csv(here::here("Data", "tweets-remotelearning.csv"))
View(tweets_rl)
tidy_tweets <- tibble(tweets_rl)
tweets_rl <- tibble(read_csv(here::here("Data", "tweets-remotelearning.csv")))
View(tweets_rl)
tweets_rl
library(tidytext)
tweet_words <- tweets_rl %>%
select(tweetid, screen_name, text, created_date) %>%
unnest_tokens(word, text)
View(tweets_rl)
tweet_words <- tweets_rl %>%
select(statusid, screen_name, text, created_date) %>%
unnest_tokens(word, text)
tweet_words
tweet_words <- tweets_rl %>%
select(status_id, screen_name, text, created_at) %>%
unnest_tokens(word, text)
tweet_words
stop_words
my_stop_words <- tibble(
word = c(
"https",
"t.co",
"rt",
"amp",
"rstats",
"gt"
),
lexicon = "twitter"
)
View(my_stop_words)
all_stop_words <- stop_words %>%
bind_rows(my_stop_words)
suppressWarnings({
no_numbers <- tweet_words %>%
filter(is.na(as.numeric(word)))
})
no_stop_words <- no_numbers %>%
anti_join(all_stop_words, by = "word")
tibble(
total_words = nrow(tweet_words),
after_cleanup = nrow(no_stop_words)
)
top_words <- no_stop_words %>%
group_by(word) %>%
tally %>%
arrange(desc(n)) %>%
head(10)
top_words
nrc_words <- no_stop_words %>%
inner_join(get_sentiments("nrc"), by = "word")
nrc_words
nrc_words %>%
group_by(sentiment) %>%
tally %>%
arrange(desc(n))
nrc_words %>%
group_by(tweetid) %>%
tally %>%
ungroup %>%
count %>%
pull
nrc_words %>%
group_by(status_id) %>%
tally %>%
ungroup %>%
count %>%
pull
library(ggjoy)
install.packages("ggjoy")
library(ggjoy)
ggplot(nrc_words) +
geom_joy(aes(
x = created_date,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "Twitter #rstats sentiment analysis",
x = "Tweet Date",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
ggplot(nrc_words) +
geom_joy(aes(
x = created_at,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "Twitter #rstats sentiment analysis",
x = "Tweet Date",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
library(RColorBrewer)
library(wordcloud)
set.seed(10)
joy_words <- nrc_words %>%
filter(sentiment == "joy") %>%
group_by(word) %>%
tally
joy_words %>%
with(wordcloud(word, n, max.words = 50, colors =  c("#56B4E9", "#E69F00")))
other_words <- nrc_words %>%
filter(sentiment == "joy") %>%
group_by(status_id) %>%
tally %>%
ungroup() %>%
inner_join(no_stop_words, by = "status_id")  %>%
anti_join(joy_words, by = "word") %>%
anti_join(top_words, by = "word") %>%
group_by(word) %>%
count
other_words %>%
with(wordcloud(word, nn, max.words = 30, colors =  c( "#56B4E9", "#E69F00")))
other_words %>%
with(wordcloud(word, max.words = 30, colors =  c( "#56B4E9", "#E69F00")))
ggplot(nrc_words) +
geom_joy(aes(
x = created_at,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "Twitter #rstats sentiment analysis",
x = "Tweet Date",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
# bing lexicon
bing_words <- no_stop_words %>%
inner_join(get_sentiments("bing"), by = "word")
bing_words
bing_words %>%
group_by(sentiment) %>%
tally %>%
arrange(desc(n))
bing_words %>%
group_by(status_id) %>%
tally %>%
ungroup %>%
count %>%
pull
# with bing lexicon
ggplot(bing_words) +
geom_joy(aes(
x = created_at,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "Twitter #remotelearning sentiment analysis",
subtitle = "nrc lexicon",
x = "Tweet Date",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
# with bing lexicon
ggplot(bing_words) +
geom_joy(aes(
x = created_at,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "Twitter #remotelearning sentiment analysis",
subtitle = "tidytext / get_sentiments() / bing lexicon",
x = "Tweet Date",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
# with bing lexicon
ggplot(bing_words) +
geom_joy(aes(
x = created_at,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "Twitter #remotelearning sentiment analysis",
subtitle = "tidytext / get_sentiments() with bing lexicon",
x = "Tweet Date",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
# with nrc lexicon
ggplot(nrc_words) +
geom_joy(aes(
x = created_at,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "Twitter #remotelearning sentiment analysis",
subtitle = "tidytext / get_sentiments() / NRC lexicon",
x = "Tweet Date",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
# with nrc lexicon
ggplot(nrc_words) +
geom_joy(aes(
x = created_at,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "Twitter #remotelearning sentiment analysis",
subtitle = "tidytext / get_sentiments() / NRC lexicon",
x = "Date of tweet",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
ggsave(here("remotelearning_nrc_lexicon.png"),
dpi = 300,  width = 11, height = 8)
ggsave(here("remotelearning_bing_lexicon.png"),
dpi = 300,  width = 11, height = 8)
joy_words %>%
with(wordcloud(word, n, max.words = 50, colors =  c("#56B4E9", "#E69F00")))
# with nrc lexicon
ggplot(nrc_words) +
geom_joy(aes(
x = created_at,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "#remotelearning sentiment analysis",
subtitle = "tidytext / get_sentiments() / NRC lexicon",
x = "Date of tweet",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
other_words %>%
with(wordcloud(word, n, max.words = 30, colors =  c( "#56B4E9", "#E69F00")))
positive_words <- nrc_words %>%
filter(sentiment == "positive") %>%
group_by(word) %>%
tally
positive_words %>%
with(wordcloud(word, n, max.words = 50, colors =  c("#56B4E9", "#E69F00")))
# with bing lexicon
ggplot(bing_words) +
geom_joy(aes(
x = created_at,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "#remotelearning sentiment analysis",
subtitle = "tidytext / get_sentiments() / bing lexicon",
x = "Date of tweet",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
positive_words_bing <- bing_words %>%
filter(sentiment == "positive") %>%
group_by(word) %>%
tally
positive_words_bing %>%
with(wordcloud(word, n, max.words = 50, colors =  c("#56B4E9", "#E69F00")))
negative_words_bing <- bing_words %>%
filter(sentiment == "negative") %>%
group_by(word) %>%
tally
negative_words_bing %>%
with(wordcloud(word, n, max.words = 50, colors =  c("#56B4E9", "#E69F00")))
# with nrc lexicon
ggplot(nrc_words) +
geom_joy(aes(
x = created_at,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "#remotelearning sentiment analysis",
subtitle = "tidytext / get_sentiments() / NRC lexicon",
x = "Date of tweet",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
ggsave(here("remotelearning_nrc_lexicon.png"),
dpi = 300,  width = 11, height = 8)
# with bing lexicon
ggplot(bing_words) +
geom_joy(aes(
x = created_at,
y = sentiment,
fill = sentiment),
rel_min_height = 0.01,
alpha = 0.7,
scale = 3) +
theme_joy() +
labs(title = "#remotelearning sentiment analysis",
subtitle = "tidytext / get_sentiments() / bing lexicon",
x = "Date of tweet",
y = "Sentiment") +
scale_fill_discrete(guide=FALSE)
ggsave(here("remotelearning_bing_lexicon.png"),
dpi = 300,  width = 11, height = 8)
positive_words_nrc %>%
with(wordcloud(word, n, max.words = 50, colors =  c("#56B4E9", "#E69F00")))
positive_words_nrc <- nrc_words %>%
filter(sentiment == "positive") %>%
group_by(word) %>%
tally
positive_words_nrc %>%
with(wordcloud(word, n, max.words = 50, colors =  c("#56B4E9", "#E69F00")))
positive_words_bing <- bing_words %>%
filter(sentiment == "positive") %>%
group_by(word) %>%
tally
positive_words_bing %>%
with(wordcloud(word, n, max.words = 50, colors =  c("#56B4E9", "#E69F00")))
## load saved results
tweets <- tibble(read_csv(here::here("Data", "tweets-remotelearning.csv")))
library(tidyverse)
## search for live tweets
# tweets <- search_tweets(q = "#remotelearning", n = 18000, include_rts = FALSE, `-filter` = "replies", lang = "en")
## load saved results
tweets <- tibble(read_csv(here::here("Data", "tweets-remotelearning.csv")))
## search for live tweets
# tweets <- search_tweets(q = "#remotelearning", n = 18000, include_rts = FALSE, `-filter` = "replies", lang = "en")
## load saved results
tweets <- tibble(read_csv(here::here("Data", "tweets-remotelearning.csv")))
astronauts <- readr::read_csv(here::here("Data", "tweets-remotelearning.csv"))
tweets
astronauts
## search for live tweets
# tweets <- search_tweets(q = "#remotelearning", n = 18000, include_rts = FALSE, `-filter` = "replies", lang = "en")
## load saved results
tweets <- readr::read_csv(here::here("Data", "tweets-remotelearning.csv"))
library(tidyverse)
library(tidytext)
library(purrrlyr)
n_word <- 20
n_top <- 150
n_gramming <- 3
tri_csv <- tibble(read_csv(here::here("Data", "frequent_trigrams.csv")))
## Trigrams
trigrams <- tibble(text = tri_csv) %>%
unnest_tokens(trigram, text, token = "ngrams", n = n_gramming)
start_words <- c("covid", "death")
trigrams <- tibble(read_csv(here::here("Data", "frequent_trigrams.csv")))
pattern <- str_c("^", start_words, " ", collapse = "|")
top_words <- trigrams %>%
filter(str_detect(trigram, pattern)) %>%
count(trigram, sort = TRUE) %>%
slice(seq_len(n_top)) %>%
pull(trigram)
## Trigrams
trigrams <- tibble(text = tri_csv) %>%
unnest_tokens(, text, token = "ngrams", n = n_gramming)
freq_terms <- tibble(read_csv(here::here("Data", "frequent_terms.csv"))) %>%
unnest_tokens(word, text)
library(tidyverse)
freq_terms <- tibble(read_csv(here::here("Data", "frequent_terms.csv")))
freq_terms  %>%
count(term, sort = TRUE) %>%
filter(n > 5000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
freq_terms  %>%
count(term, sort = TRUE) %>%
filter(n > 5000) %>%
mutate(term = reorder(term, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
freq_terms  %>%
count(term, sort = TRUE) %>%
filter(n > 5000) %>%
mutate(term = reorder(term, n)) %>%
ggplot(aes(term, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
freq_terms  %>%
count(term, sort = TRUE) %>%
filter(n > 500000) %>%
mutate(term = reorder(term, n)) %>%
ggplot(aes(term, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
library(tidytext)
freq_terms  %>%
count(term, sort = TRUE) %>%
filter(n > 500000) %>%
mutate(term = reorder(term, n)) %>%
ggplot(aes(term, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
tidy_text <- train %>% unnest_tokens(term, text)
tidy_text <- freq_terms %>% unnest_tokens(term, text)
freq_terms
freq_terms
freq_termss <- tibble(read_csv(here::here("Data", "frequent_terms.csv"))),
col_types = cols(
term = col_character(),
count= col_integer()
))
freq_termss <- tibble(read_csv(here::here("Data", "frequent_terms.csv"))),
col_types = cols(
term = col_character(),
count= col_integer())
freq_termss
freq_termss <- tibble(read_csv(here::here("Data", "frequent_terms.csv"))),
col_types = cols(
term = col_character(),
count= col_integer())
freq_termss <- tibble(read_csv(here::here("data", "frequent_terms.csv"))),
col_types = cols(
term = col_character(),
count= col_integer())
freq_termss <- tibble(read_csv(here::here("data", "frequent_terms.csv")))
View(freq_terms)
freq_termss  %>%
filter(n > 500000) %>%
ggplot(aes(term, count())) +
geom_col() +
xlab(NULL) +
coord_flip()
freq_termss  %>%
filter(count > 500000) %>%
ggplot(aes(term, count())) +
geom_col() +
xlab(NULL) +
coord_flip()
freq_termss  %>%
filter(count > 500000) %>%
ggplot(aes(term, count)) +
geom_col() +
xlab(NULL) +
coord_flip()
freq_termss  %>%
filter(count > 600000) %>%
ggplot(aes(term, count)) +
geom_col() +
xlab(NULL) +
coord_flip()
freq_termss  %>%
filter(count > 800000) %>%
ggplot(aes(term, count)) +
geom_col() +
xlab(NULL) +
coord_flip()
freq_termss  %>%
filter(count > 1000000) %>%
ggplot(aes(term, count)) +
geom_col() +
xlab(NULL) +
coord_flip()
freq_termss  %>%
filter(count > 1200000) %>%
ggplot(aes(term, count)) +
geom_col() +
xlab(NULL) +
coord_flip()
freq_termss  %>%
filter(count > 1300000) %>%
ggplot(aes(term, count)) +
geom_col() +
xlab(NULL) +
coord_flip()
